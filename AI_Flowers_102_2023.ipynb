{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CourtneyBrookes/ProblemSet3/blob/main/AI_Flowers_102_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flowers 102"
      ],
      "metadata": {
        "id": "rs8UhCRsuBDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(x,title=None):\n",
        "    # Move tensor to CPU and convert to numpy\n",
        "    x_np = x.cpu().numpy()\n",
        "\n",
        "    # If tensor is in (C, H, W) format, transpose to (H, W, C)\n",
        "    if x_np.shape[0] == 3 or x_np.shape[0] == 1:\n",
        "        x_np = x_np.transpose(1, 2, 0)\n",
        "\n",
        "    # If grayscale, squeeze the color channel\n",
        "    if x_np.shape[2] == 1:\n",
        "        x_np = x_np.squeeze(2)\n",
        "\n",
        "    x_np = x_np.clip(0, 1)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    if len(x_np.shape) == 2:  # Grayscale\n",
        "        im = ax.imshow(x_np, cmap='gray')\n",
        "    else:\n",
        "        im = ax.imshow(x_np)\n",
        "    plt.title(title)\n",
        "    ax.axis('off')\n",
        "    fig.set_size_inches(10, 10)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fdJFcSO0DrMU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and extracting the dataset\n",
        "# Uncomment the following lines if you are running this in a Jupyter Notebook\n",
        "!wget https://gist.githubusercontent.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1/raw/403325f5110cb0f3099734c5edb9f457539c77e9/Oxford-102_Flower_dataset_labels.txt\n",
        "!wget https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
        "!unzip 'flower_data.zip'"
      ],
      "metadata": {
        "id": "9bIG_DcPxI4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Directory and transforms\n",
        "data_dir = '/content/flower_data/'\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load the dataset using ImageFolder\n",
        "dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transform)\n",
        "dataset_labels = pd.read_csv('Oxford-102_Flower_dataset_labels.txt', header=None)[0].str.replace(\"'\", \"\").str.strip()\n",
        "\n",
        "# Load the dataset into a DataLoader for batching\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)"
      ],
      "metadata": {
        "id": "AKde_7VoCBB1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the batch of images and labels\n",
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "print(f\"Images tensor shape: {images.shape}\")\n",
        "print(f\"Labels tensor shape: {labels.shape}\")\n"
      ],
      "metadata": {
        "id": "yNmdFAMjEOar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8723f9-00b3-4601-f809-c627c1bfcc1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images tensor shape: torch.Size([6552, 3, 224, 224])\n",
            "Labels tensor shape: torch.Size([6552])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 50\n",
        "plot(images[i],dataset_labels[i]);"
      ],
      "metadata": {
        "id": "OEfJHtNqCOBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ubt-N8qdlf15"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#define alexnet model\n",
        "alexnet = models.alexnet(pretrained=True).to(device)\n",
        "labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()}\n",
        "\n",
        "#transform image for use in model\n",
        "preprocess = transforms.Compose([\n",
        "   transforms.Resize(256),\n",
        "   transforms.CenterCrop(224),\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "Rtaj-tsed8iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = images[i]"
      ],
      "metadata": {
        "id": "SmetTIj6mDFn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "to_pil = ToPILImage()\n",
        "img = to_pil(img)"
      ],
      "metadata": {
        "id": "7k8RESQsexB4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t = preprocess(img).unsqueeze_(0).to(device)"
      ],
      "metadata": {
        "id": "XkTMbUhidqQE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t.shape"
      ],
      "metadata": {
        "id": "q2JJuEmCHMB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1117174-d49c-40aa-97d7-739003c39c50"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labels"
      ],
      "metadata": {
        "id": "rmv1DMuuHXxI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify the image with alexnet\n",
        "scores, class_idx = alexnet(img_t).max(1)\n",
        "print('Predicted class:', labels[class_idx.item()])"
      ],
      "metadata": {
        "id": "mgpHseEHHKSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccdae46d-054a-496b-9c5e-06f80ae3e755"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: sarong\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = alexnet.features[0].weight.data\n",
        "w1 = alexnet.features[3].weight.data\n",
        "w2 = alexnet.features[6].weight.data\n",
        "w3 = alexnet.features[8].weight.data\n",
        "w4 = alexnet.features[10].weight.data\n",
        "w5 = alexnet.classifier[1].weight.data\n",
        "w6 = alexnet.classifier[4].weight.data\n",
        "w7 = alexnet.classifier[6].weight.data"
      ],
      "metadata": {
        "id": "I9jMV0nSvW-6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and Load\n",
        "# w = [w0,w1,w2,w3,w4,w5,w6,w7]\n",
        "# torch.save(w, 'Hahn_Alex.pt')\n",
        "# w = torch.load('Hahn_Alex.pt')\n",
        "# [w0,w1,w2,w3,w4,w5,w6,w7] = w\n",
        "# [w0,w1,w2,w3,w4,w5,w6,w7] = torch.load('Hahn_Alex.pt')"
      ],
      "metadata": {
        "id": "Bo4sKueVvXBY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_t.shape,w0.shape"
      ],
      "metadata": {
        "id": "NTghf1WV74of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591ae6a9-5a34-43cd-82fe-5163680efef8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 224, 224]), torch.Size([64, 3, 11, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_t.shape"
      ],
      "metadata": {
        "id": "YYbON_3XJr3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732c0c05-89c8-460d-9d81-afaf898becac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_t[0,:,:,:].shape"
      ],
      "metadata": {
        "id": "Zh143pB2KAoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a9c851-9933-4c9c-bc3f-fc56a9afa900"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scale(img):\n",
        "    # Normalize the NumPy array to the range [0, 1]\n",
        "    max_value = img.max()\n",
        "    min_value = img.min()\n",
        "    normalized_array = (img - min_value) / (max_value - min_value)\n",
        "    return normalized_array"
      ],
      "metadata": {
        "id": "fupA6inAMMWO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_plot(img_t,index=0):\n",
        "    numpy_array = img_t[index,:,:,:].cpu().numpy()\n",
        "    numpy_array_transposed = numpy_array.transpose(1, 2, 0)\n",
        "    numpy_array_transposed = scale(numpy_array_transposed)\n",
        "    plt.imshow(numpy_array_transposed)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "E45WF77_LNG3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_plot(img_t)"
      ],
      "metadata": {
        "id": "MnLVNs_CLWnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0.shape"
      ],
      "metadata": {
        "id": "_GJy-69VLw53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97865e8d-fad1-4869-8ec0-661123d574f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 3, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f0 = F.conv2d(img_t, w0, stride=4, padding=2)"
      ],
      "metadata": {
        "id": "0dhT4aowvXEY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f0.shape"
      ],
      "metadata": {
        "id": "iOIIu6zJHyjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124ebfb3-de07-4728-bd61-b087122a545a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 55, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "plt.imshow(f0[0,i,:,:].cpu().numpy())"
      ],
      "metadata": {
        "id": "uFGvWefmNh43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(64):\n",
        "#     tensor_plot(w0,i)\n",
        "#     plt.imshow(f0[0,i,:,:].cpu().numpy())\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "-KwGtc8wM7DM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_feature_maps_with_filters(feature_maps, filters):\n",
        "    # Remove batch dimension if it exists\n",
        "    if feature_maps.dim() == 4:\n",
        "        feature_maps = feature_maps.squeeze(0)\n",
        "\n",
        "    # Normalize feature maps to [0, 1]\n",
        "    feature_maps = (feature_maps - feature_maps.min()) / (feature_maps.max() - feature_maps.min())\n",
        "\n",
        "    def add_filter_to_feature_map(filter_tensor, feature_map_tensor):\n",
        "        # Ensure the feature map is 2D [H, W]\n",
        "        if feature_map_tensor.dim() > 2:\n",
        "            feature_map_tensor = feature_map_tensor.squeeze(0)\n",
        "\n",
        "        # Convert grayscale feature map to RGB by repeating the single channel 3 times\n",
        "        feature_map_rgb = feature_map_tensor.unsqueeze(0).repeat((3, 1, 1))\n",
        "\n",
        "        # Normalize the filter to [0, 1]\n",
        "        filter_tensor = (filter_tensor - filter_tensor.min()) / (filter_tensor.max() - filter_tensor.min())\n",
        "\n",
        "        # Ensure the filter fits into the feature map\n",
        "        min_dim = min(feature_map_tensor.shape)\n",
        "        filter_size = min(filter_tensor.shape[-1], min_dim)\n",
        "\n",
        "        # Crop the filter if needed\n",
        "        filter_cropped = filter_tensor[:, :filter_size, :filter_size]\n",
        "\n",
        "        # Overlay the RGB filter at the lower-left corner of the feature map\n",
        "        feature_map_rgb[:, -filter_size:, :filter_size] = filter_cropped\n",
        "\n",
        "        # Clip the values to be in the range [0, 1]\n",
        "        feature_map_rgb = torch.clamp(feature_map_rgb, 0, 1)\n",
        "\n",
        "        return feature_map_rgb\n",
        "\n",
        "    # Plot montage of feature maps\n",
        "    fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
        "\n",
        "    for ax, feature_map, filter_ in zip(axes.flat, feature_maps, filters):\n",
        "        # Add RGB filter to grayscale feature map\n",
        "        modified_feature_map = add_filter_to_feature_map(filter_, feature_map)\n",
        "\n",
        "        # Plot modified feature map\n",
        "        ax.imshow(modified_feature_map.permute(1, 2, 0).cpu().numpy(), interpolation='none')  # Added 'none' interpolation\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qkMqr835N-W2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f0.shape,w0.shape"
      ],
      "metadata": {
        "id": "Ug6cetACrtzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c045af7-3569-43c8-9d2a-3e8de70e3f8c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64, 55, 55]), torch.Size([64, 3, 11, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_feature_maps_with_filters(f0, w0)"
      ],
      "metadata": {
        "id": "mQb-EZfpq-N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2pdFdqOlf4I"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}